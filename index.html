<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yashwanth Krishna Mothukuri">

<title>IDC6940_week3</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">IDC6940_week3</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Yashwanth Krishna Mothukuri </p>
          </div>
  </div>
    
  
    
  </div>
  


</header>


<section id="backgroundmotivation" class="level2">
<h2 class="anchored" data-anchor-id="backgroundmotivation">Background/Motivation</h2>
<section id="context-of-the-text" class="level4">
<h4 class="anchored" data-anchor-id="context-of-the-text">Context of the text:</h4>
<p>Research studies predictive process analytics by examining how machine learning predicts future business process states through determining ongoing process instance outcomes. The increased precision of XGBoost alongside other advanced machine learning techniques introduces interpretability challenges to processing models which become “black box” structures. Buildings models as transparent systems through Explainable Artificial Intelligence (XAI) methods becomes essential due to black box conditions in these predictive systems. XAI methods including LIME and SHAP help interpret black-box model predictions in the area of predictive process analytics through this study.</p>
</section>
<section id="problem-or-gap-in-the-existing-literature" class="level4">
<h4 class="anchored" data-anchor-id="problem-or-gap-in-the-existing-literature">Problem or Gap in the Existing Literature:</h4>
<p>Earlier research has employed generic XAI methods including LIME and SHAP in predictive process analytics but it lacks comprehensive quality evaluation of explanations. XAI assessment methods exhibit limited universality since they work exclusively for specific methods or one of three data types: text or images or tabular data. The analysis of tabular data from event logs which forms standard input for process analytics reveals a major deficiency in the system. The evaluation approaches for determining model fidelity through understanding how explanations match decision-making systems and stability through explaining similar inputs face three significant challenges because the applied methods often vary between specific models and fail to work with process-based data.</p>
</section>
<section id="significance-of-the-research-question" class="level4">
<h4 class="anchored" data-anchor-id="significance-of-the-research-question">Significance of the Research Question:</h4>
<p>Predictive process analytics depends on explainable reasoning knowledge to improve user trust together with interface accessibility. This investigation has closed the evaluation gap by developing metrics to assess XAI methods when focusing on process prediction functionality. The measurement of stability and fidelity represents essential criteria to determine reliable explainable methods particularly in settings where process-based decisions generate meaningful real-world effects. This research delivers usable observations about improving decision-making systems while advancing business applications of interpretable machine learning method.</p>
</section>
</section>
<section id="methods-used" class="level2">
<h2 class="anchored" data-anchor-id="methods-used">Methods Used:</h2>
<section id="summary-of-the-methodologies-employed-by-the-authors" class="level4">
<h4 class="anchored" data-anchor-id="summary-of-the-methodologies-employed-by-the-authors">Summary of the methodologies employed by the authors</h4>
<p>Author Velmurugan et al explore explainable methods for predictive process analytics through functionally-grounded evaluation metrics in their paper. The methodologies employed are as follows: Explainability Metrics: The authors propose two key metrics for evaluating the quality of explanations: Stability: Total consistency across recurring or equivalent input cases is what stability evaluates. Fidelity: This evaluation method determines how accurately an explanation shows the underlying decision path performed by black-box models.</p>
<section id="evaluation-design" class="level5">
<h5 class="anchored" data-anchor-id="evaluation-design">Evaluation Design:</h5>
<p>Three open-source real-world event log datasets served as the foundation for evaluation purposes. Each process instance received multiple explanations to enable researchers to measure both stability and fidelity.</p>
</section>
<section id="machine-learning-models" class="level5">
<h5 class="anchored" data-anchor-id="machine-learning-models">Machine Learning Models:</h5>
<p>Researchers used XGBoost as an algorithm to build predictive models because of its high prediction accuracy in process outcome analysis. The models received various combinations of data encoding techniques between aggregate and index-based joined by prefix-length bucketing and no bucketing approaches.</p>
</section>
<section id="explanation-techniques" class="level5">
<h5 class="anchored" data-anchor-id="explanation-techniques">Explanation Techniques:</h5>
<p>Predictions were interpreted by implementing two widely used explainable AI techniques: LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations).</p>
</section>
<section id="quantitative-measures" class="level5">
<h5 class="anchored" data-anchor-id="quantitative-measures">Quantitative Measures:</h5>
<p>Stability was evaluated using two measures: Stability by Subset (ϕ(Z)ϕ(Z)): A mathematical formula deploys sample variance to measure how closely related different feature subsets remain between explanations. Stability by Weight (ϕ(W)ϕ(W)): Using relative variance it measures how consistent feature importance weights remain throughout interpretation explanations. The interpretation fidelity measurement utilized Mean Absolute Percentage Error (MAPE) to determine the accuracy of the interpreter against the black-box system’s operational process.</p>
</section>
</section>
<section id="suitability-of-methods-to-address-research-questions" class="level4">
<h4 class="anchored" data-anchor-id="suitability-of-methods-to-address-research-questions">Suitability of Methods to Address Research Questions:</h4>
<p>The proposed methods are well-suited to address the research question of assessing the trustworthiness of explainable AI methods in predictive process analytics:</p>
<section id="functionally-grounded-metrics" class="level5">
<h5 class="anchored" data-anchor-id="functionally-grounded-metrics">Functionally-Grounded Metrics:</h5>
<p>The evaluation uses stability and fidelity assessment methods which eliminate user-based tests for an objective framework resulting in precise reproducible results.</p>
</section>
<section id="use-of-xgboost" class="level5">
<h5 class="anchored" data-anchor-id="use-of-xgboost">Use of XGBoost:</h5>
<p>The effectiveness of explainable methods can be effectively tested using XGBoost which demonstrates high accuracy in predictive assignments.</p>
</section>
<section id="real-world-datasets" class="level5">
<h5 class="anchored" data-anchor-id="real-world-datasets">Real-World Datasets:</h5>
<p>The analysis of real-world event logs enables researchers to establish findings which translate effectively to business process operations.</p>
</section>
</section>
<section id="noteworthy-innovative-approaches" class="level4">
<h4 class="anchored" data-anchor-id="noteworthy-innovative-approaches">Noteworthy Innovative Approaches</h4>
<section id="functionally-grounded-evaluation-metrics" class="level5">
<h5 class="anchored" data-anchor-id="functionally-grounded-evaluation-metrics">Functionally-Grounded Evaluation Metrics:</h5>
<p>This paper develops specific evaluation metrics for event log datasets to overcome the deficiencies of standard XAI assessment tools which were created for texts and images.</p>
</section>
<section id="stability-metrics" class="level5">
<h5 class="anchored" data-anchor-id="stability-metrics">Stability Metrics:</h5>
<p>• Stability by Subset <span class="math inline">\(\phi(Z)\)</span> measures similarity in feature subsets across explanations using the following formula: <span class="math display">\[\phi(Z)={1-1/d} \sum^d_{i=1} s^{2}_{f_i}/ k.d(1-k/d)\]</span></p>
<ol type="1">
<li><p>• where d is the number of features, <span class="math inline">\(s^2_{f_i}\)</span> is sample variance, and k is the average number of selected features. • Stability by Weight <span class="math inline">\(\phi(W)\)</span> measures consistency in feature weights: <span class="math display">\[ \phi(W)={1-1/d} \sum^d_{i=1} \sigma^{2}{w_i}/ |\mu{w_i}|\]</span> where <span class="math inline">\(\sigma^2_{w_i}\)</span> is variance and <span class="math inline">\(\mu_{w_i}\)</span> is mean weight for feature ii.</p></li>
<li><p>Fidelity Metric: • Internal fidelity is quantified using MAPE:<span class="math display">\[ F=1/|X'| \sum_{x'\epsilon X'} |Y(x)-Y(x')|/Y(x) \]</span> where Y(x)and Y(x′) are prediction probabilities for original and perturbed inputs, respectively.</p></li>
<li><p>Combination of Encoding and Bucketing Methods: • The use of aggregate encoding with prefix-length bucketing or no bucketing provides insights into how data preprocessing impacts explainability.</p></li>
</ol>
</section>
</section>
<section id="key-findings-and-contributions" class="level4">
<h4 class="anchored" data-anchor-id="key-findings-and-contributions">Key Findings and Contributions</h4>
<section id="novel-evaluation-framework" class="level5">
<h5 class="anchored" data-anchor-id="novel-evaluation-framework">1. Novel Evaluation Framework</h5>
<p>A functionally-grounded evaluation system receives special endorsement from the authors to assess explainable methods in predictive process analytics. Existing research lacked evaluation methods which specifically supported assessable explainable methods across different prediction problems according to this proposed framework.</p>
</section>
<section id="stability-metrics-1" class="level5">
<h5 class="anchored" data-anchor-id="stability-metrics-1">2.Stability Metrics</h5>
<p>Two stability metrics were introduced:</p>
<ol type="a">
<li>Stability by Subset (φ(Z)): <span class="math display">\[\phi(Z)={1-1/d} \sum^d_{i=1} s^{2}_{f_i}/ k.d(1-k/d)\]</span> This metric measures the consistency of feature subsets across explanations.</li>
<li>Stability by Weight (φ(W)): <span class="math display">\[ \phi(W)={1-1/d} \sum^d_{i=1} \sigma^{2}{w_i}/ |\mu{w_i}|\]</span> This metric assesses the stability of feature importance weights.</li>
</ol>
<!-- -->
</section>
<section id="internal-fidelity-metric" class="level5">
<h5 class="anchored" data-anchor-id="internal-fidelity-metric">3.Internal Fidelity Metric</h5>
<p>The authors propose a fidelity measure (F) based on Mean Absolute Percentage Error: <span class="math display">\[ F=1/|X'| \sum_{x'\epsilon X'} |Y(x)-Y(x')|/Y(x) \]</span></p>
<p>This metric quantifies how faithfully the explanation reflects the black-box model’s decision-making process.</p>
</section>
<section id="comprehensive-evaluation" class="level5">
<h5 class="anchored" data-anchor-id="comprehensive-evaluation">4. Comprehensive Evaluation:</h5>
<p>The researchers examine LIME and SHAP as popular explainable AI techniques for their interpretation of XGBoost models that predict process outcomes. Three actual event log datasets served as research platforms to assess these methods in real-world execution settings.</p>
</section>
<section id="data-encoding-and-bucketing-analysis" class="level5">
<h5 class="anchored" data-anchor-id="data-encoding-and-bucketing-analysis">5. Data Encoding and Bucketing Analysis:</h5>
<p>Researchers examine how various data encoding methods (aggregate and index-based) alongside bucketing techniques (prefix-length bucketing and no bucketing) affect explanation quality.</p>
</section>
</section>
<section id="significance-in-the-broader-context" class="level4">
<h4 class="anchored" data-anchor-id="significance-in-the-broader-context">Significance in the Broader Context</h4>
<section id="addressing-the-transparency-gap" class="level5">
<h5 class="anchored" data-anchor-id="addressing-the-transparency-gap">Addressing the Transparency Gap</h5>
<p>Due to increasing reliance on complex machine learning models in predictive process analytics the landscape demands stronger transparency and interpretability features. The study specifically aims to solve this problem by offering an assessment framework for clarifying techniques which promotes both AI-driven process analytics transparency and trustworthiness.</p>
</section>
<section id="standardization-of-evaluation" class="level5">
<h5 class="anchored" data-anchor-id="standardization-of-evaluation">Standardization of Evaluation</h5>
<p>The research introduces performance metrics that depend on functional quality to standardize explainable method assessments in process analytics. Standardization provides essential tools for field advancement through method comparison.</p>
</section>
<section id="bridging-theory-and-practice" class="level5">
<h5 class="anchored" data-anchor-id="bridging-theory-and-practice">Bridging Theory and Practice</h5>
<p>Real-world dataset analysis integrations with XGBoost machine learning techniques drive forces between theoretical explainable AI learning and actual process analytics implementations. The applied methods deliver instant results that benefit starkly practical implementation across industry. Foundation for User-Oriented Evaluation</p>
<p>The research creates essential foundational work which will enable upcoming human user evaluations. Effective user studies depend on comprehensive comprehension of technical explanation quality.</p>
</section>
<section id="guidance-for-practitioners" class="level5">
<h5 class="anchored" data-anchor-id="guidance-for-practitioners">Guidance for Practitioners</h5>
<p>This research evaluates how two common explainable methods (LIME and SHAP) perform when used in process analytics applications. The identified guidelines offer crucial help to practitioners who select methodologies based on their individual requirements.</p>
</section>
<section id="data-preprocessing-insights" class="level5">
<h5 class="anchored" data-anchor-id="data-preprocessing-insights">Data Preprocessing Insights</h5>
<p>Data preprocessing insights become clearer through the examination of various bucketing and encoding methods which affects explainability outputs. Designing predictive process analytics systems requires effective preprocessing decisions which directly influence both explanation quality and interpretability.</p>
</section>
<section id="extensibility-and-future-research" class="level5">
<h5 class="anchored" data-anchor-id="extensibility-and-future-research">Extensibility and Future Research</h5>
<p>Two key extension capabilities of the designed metrics and evaluation framework enable their application to new explainable methods that arise during predictive process analytics research. The research will maintain its value because this proactive outlook means it stays aligned with current needs.</p>
</section>
<section id="interdisciplinary-impact" class="level5">
<h5 class="anchored" data-anchor-id="interdisciplinary-impact">Interdisciplinary Impact</h5>
<p>This research adds value to explainable artificial intelligence by showing how specific process analytics parameters can enhance XAI evaluation frameworks. This method sets an example which might inspire parallel domain-based evaluation techniques across various fields.</p>
</section>
</section>
<section id="implications-of-the-findings-for-future-research-or-practice" class="level4">
<h4 class="anchored" data-anchor-id="implications-of-the-findings-for-future-research-or-practice">Implications of the findings for future research or practice:</h4>
<section id="standardization-of-evaluation-metrics" class="level5">
<h5 class="anchored" data-anchor-id="standardization-of-evaluation-metrics">Standardization of Evaluation Metrics</h5>
<p>The proposed functionally-grounded metrics for stability and fidelity provide a foundation for standardizing the evaluation of explainable methods in process analytics. These metrics include: Stability by Subset, Stability by Weight, Internal Fidelity</p>
<p>The newly developed metrics facilitate better evaluation of explainable approaches leading to faster field advancement.</p>
<p>Data Preprocessing Considerations</p>
<p>Tests show that how data encoding and bucketing practices function impacts explanation quality. The study explored three combinations:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Encoding Method</th>
<th>Bucketing Method</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Aggregate</td>
<td>Prefix-length</td>
</tr>
<tr class="even">
<td>Index-based</td>
<td>Prefix-length</td>
</tr>
<tr class="odd">
<td>Aggregate</td>
<td>No bucketing</td>
</tr>
</tbody>
</table>
<p>Proper preprocessing within predictive process analytics pipelines requires immediate attention because of the discovered inconsistencies. Research on the discovery of optimal preprocessing techniques for varying process types and predictive challenges should be a focus for upcoming studies.</p>
</section>
<section id="expanding-evaluation-frameworks" class="level5">
<h5 class="anchored" data-anchor-id="expanding-evaluation-frameworks">Expanding Evaluation Frameworks</h5>
<p>While this study focuses on stability and fidelity, future research could expand the evaluation framework to include other dimensions such as Comprehensibility, Action ability, Computational efficiency</p>
<p>Future research using this functionally-grounded approach should conduct assessments with end-users to evaluate explanation capabilities in real-world practical usage.</p>
</section>
<section id="integration-with-process-mining" class="level5">
<h5 class="anchored" data-anchor-id="integration-with-process-mining">Integration with Process Mining</h5>
<p>The investigation of future research should study methods through which explainable predictive analytics can combine with process discovery techniques to deliver complete business process insights. The merger between explainable predictive models and process mining techniques has the potential to create an integrated approach to analysis and process transformation.</p>
</section>
<section id="ethical-and-responsible-ai" class="level5">
<h5 class="anchored" data-anchor-id="ethical-and-responsible-ai">Ethical and Responsible AI</h5>
<p>The evaluation framework shows promise for adding fairness analysis alongside bias detection for process predictions and explanations while establishing responsible AI standards for process management. The increasing regulatory focus on AI explainability demands a foundation which will enable the development of dedicated compliance frames for predictive process analytics.</p>
<p>This research simultaneously pushes forward existing explainable predictive process analytical methods while establishing multiple future research possibilities and practical usage avenues. Researchers together with practitioners must focus on these regions to develop trustworthy along with transparent AI-driven process analytics solutions which provide effective results.</p>
</section>
</section>
<section id="connection-to-other-work" class="level4">
<h4 class="anchored" data-anchor-id="connection-to-other-work">Connection to Other Work</h4>
<p>This paper by Velmurugan et al.&nbsp;builds on and extends previous work in explainable AI and predictive process analytics in several key ways:</p>
<section id="extension-of-xai-evaluation-to-process-analytics" class="level5">
<h5 class="anchored" data-anchor-id="extension-of-xai-evaluation-to-process-analytics">Extension of XAI Evaluation to Process Analytics</h5>
<p>The authors applied existing evaluation frameworks for XAI to predictive process analytics while making adjustable modifications to better fit this domain. Building from Doshi-Velez and Kim’s (2017) three-level evaluation framework they concentrate their analysis on functionally-grounded evaluation practices. The authors extend existing XAI evaluation practices through applying their principles to specific process data requirements.</p>
<p>Novel Stability Metrics</p>
<p>The paper introduces two stability metrics adapted for process analytics:</p>
<p>Stability by Subset (φ(Z)): The metric utilizes modifications made to Nogueira et al.&nbsp;(2018) feature selection stability framework which operates within explainable process analytics boundaries.</p>
<p>Stability by Weight (φ(W)): The newly developed metric calculates feature weight stability through relative variance analysis.</p>
<p>These metrics extend existing stability measurements from XAI fields by specifically supporting tabular structures and time-related characteristics of process data.</p>
</section>
<section id="internal-fidelity-metric-1" class="level5">
<h5 class="anchored" data-anchor-id="internal-fidelity-metric-1">Internal Fidelity Metric</h5>
<p>The authors introduce an internal fidelity assessment that uses Mean Absolute Percentage Error (MAPE) as its foundation. This differs from previous approaches like:</p>
<p>Researchers used white-box models according to Ribeiro et al.&nbsp;(2016) to measure fidelity.</p>
<p>The researchers Alvarez-Melis and Jaakkola (2018) removed features through an ablation technique to evaluate system fidelity.</p>
<p>The newly designed computational method specializes in analyzing process analytics data while fixing the shortcomings of present measurement techniques that target images and text.</p>
</section>
<section id="comprehensive-evaluation-framework" class="level5">
<h5 class="anchored" data-anchor-id="comprehensive-evaluation-framework">Comprehensive Evaluation Framework</h5>
<p>The authors introduce a united stability-fidelity framework through their work which delivers stronger evaluation capabilities than research on explainable process analytics from Galanti et al.&nbsp;(2020) and Rehse et al.&nbsp;(2019), that solely engaged in XAI method usage without extensive evaluation.</p>
</section>
<section id="integration-with-state-of-the-art-process-prediction" class="level5">
<h5 class="anchored" data-anchor-id="integration-with-state-of-the-art-process-prediction">Integration with State-of-the-Art Process Prediction</h5>
<p>The research expands upon research by Teinemaa et al.&nbsp;(2019) which validated XGBoost as an optimal algorithm for predicting outcomes by assessing LIME and SHAP explanations for XGBoost models.</p>
</section>
<section id="data-encoding-and-bucketing-analysis-1" class="level5">
<h5 class="anchored" data-anchor-id="data-encoding-and-bucketing-analysis-1">Data Encoding and Bucketing Analysis</h5>
<p>This analysis investigates the effect of diverse encoding and bucketing approaches on explainability quality based on the work performed by Teinemaa et al.&nbsp;(2019) regarding prediction accuracy enhancements from these techniques.</p>
</section>
<section id="key-seminal-works-cited-include" class="level5">
<h5 class="anchored" data-anchor-id="key-seminal-works-cited-include">Key seminal works cited include:</h5>
<p>Doshi-Velez and Kim (2017) on XAI evaluation frameworks</p>
<p>Ribeiro et al.&nbsp;(2016) introducing LIME</p>
<p>Lundberg and Lee (2017) introducing SHAP</p>
</section>
</section>
<section id="relevance-to-capstone-project" class="level4">
<h4 class="anchored" data-anchor-id="relevance-to-capstone-project">Relevance to Capstone Project:</h4>
<p>Focus on Predictive Analytics: Historical data predictive modeling fits the paper’s predictive process analytics focus that mirrors ability to make predictions from historical information.</p>
<p>Use of Advanced Machine Learning: XGBoost serves as the primary tool for model building within this study that directly benefits project’s machine learning element.</p>
<p>Evaluation of Explainable AI Methods: A comparative analysis of the explainable AI techniques LIME and SHAP takes place within this study. Including explainable aspects in predictive models can result in major improvements for their overall reliability and worth.</p>
<section id="specific-methods-and-theories-to-incorporate" class="level5">
<h5 class="anchored" data-anchor-id="specific-methods-and-theories-to-incorporate">Specific Methods and Theories to Incorporate:</h5>
<p>Evaluation Metrics: The paper proposes functionally-grounded evaluation metrics for assessing explainable methods:</p>
<p>Stability by Subset Stability by Weight Internal Fidelity</p>
<p>Data Preprocessing Techniques: The paper presents several data encoding and bucketing approaches suitable for preparing historic data.</p>
<p>Aggregate encoding with prefix-length bucketing, Index-based encoding with prefix-length bucketing, Aggregate encoding with no bucketing</p>
<p>Use of Real-World Datasets: The research points to three open-source and real-event systems documenting all workforce activities. The use of multiple sources in authentic datasets works ideally with your historical data analytic requirements.</p>
<p>Potential Areas for Expansion or Divergence:</p>
<p>Domain-Specific Adaptations: Although a process analytics approach guides the research your team could make targeted adjustments to apply their techniques to historical data analysis.</p>
<p>Additional ML Algorithms: The paper primarily uses XGBoost. The study could benefit from extension by performing a comparison between different predictive machine learning algorithms as part of its methodology.</p>
<p>Human-Grounded Evaluation: Evaluation takes place through applying functional criteria in this paper. Your project may add human-grounded assessment techniques to evaluate how practical the explanations from your models prove in actual applications.</p>
<p>Temporal Aspects: Your predictive models require stronger focus on temporal dimensions throughout the dataset which can lead to developing innovative features and time-based analysis approaches.</p>
</section>
</section>
<section id="refrences" class="level3">
<h3 class="anchored" data-anchor-id="refrences">Refrences</h3>
<p>M. Velmurugan, C. Ouyang, C. Moreira, and R. Sindhgatta, “Evaluating explainable methods for predictive process analytics: A functionally-grounded approach,” arXiv preprint arXiv:2012.04218, 2020.</p>
<p>F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable machine learning,” arXiv preprint arXiv:1702.08608, 2017.</p>
<p>D. Alvarez-Melis and T. S. Jaakkola, “On the robustness of interpretability methods,” arXiv preprint arXiv:1806.08049, 2018.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>