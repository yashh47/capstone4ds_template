---
title: "IDC6940_week3"
author: "Yashwanth Krishna Mothukuri"
format: html
---

## Background/Motivation

#### Context of the text:

Research studies predictive process analytics by examining how machine learning predicts future business process states through determining ongoing process instance outcomes. The increased precision of XGBoost alongside other advanced machine learning techniques introduces interpretability challenges to processing models which become "black box" structures. Buildings models as transparent systems through Explainable Artificial Intelligence (XAI) methods becomes essential due to black box conditions in these predictive systems. XAI methods including LIME and SHAP help interpret black-box model predictions in the area of predictive process analytics through this study.

#### Problem or Gap in the Existing Literature:

Earlier research has employed generic XAI methods including LIME and SHAP in predictive process analytics but it lacks comprehensive quality evaluation of explanations. XAI assessment methods exhibit limited universality since they work exclusively for specific methods or one of three data types: text or images or tabular data. The analysis of tabular data from event logs which forms standard input for process analytics reveals a major deficiency in the system. The evaluation approaches for determining model fidelity through understanding how explanations match decision-making systems and stability through explaining similar inputs face three significant challenges because the applied methods often vary between specific models and fail to work with process-based data.

#### Significance of the Research Question:

Predictive process analytics depends on explainable reasoning knowledge to improve user trust together with interface accessibility. This investigation has closed the evaluation gap by developing metrics to assess XAI methods when focusing on process prediction functionality. The measurement of stability and fidelity represents essential criteria to determine reliable explainable methods particularly in settings where process-based decisions generate meaningful real-world effects. This research delivers usable observations about improving decision-making systems while advancing business applications of interpretable machine learning method.

## Methods Used:

#### Summary of the methodologies employed by the authors

Author Velmurugan et al explore explainable methods for predictive process analytics through functionally-grounded evaluation metrics in their paper. The methodologies employed are as follows: Explainability Metrics: The authors propose two key metrics for evaluating the quality of explanations: Stability: Total consistency across recurring or equivalent input cases is what stability evaluates. Fidelity: This evaluation method determines how accurately an explanation shows the underlying decision path performed by black-box models.

##### Evaluation Design:

Three open-source real-world event log datasets served as the foundation for evaluation purposes. Each process instance received multiple explanations to enable researchers to measure both stability and fidelity.

##### Machine Learning Models:

Researchers used XGBoost as an algorithm to build predictive models because of its high prediction accuracy in process outcome analysis. The models received various combinations of data encoding techniques between aggregate and index-based joined by prefix-length bucketing and no bucketing approaches.

##### Explanation Techniques:

Predictions were interpreted by implementing two widely used explainable AI techniques: LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations).

##### Quantitative Measures:

Stability was evaluated using two measures: Stability by Subset (ϕ(Z)ϕ(Z)): A mathematical formula deploys sample variance to measure how closely related different feature subsets remain between explanations. Stability by Weight (ϕ(W)ϕ(W)): Using relative variance it measures how consistent feature importance weights remain throughout interpretation explanations. The interpretation fidelity measurement utilized Mean Absolute Percentage Error (MAPE) to determine the accuracy of the interpreter against the black-box system's operational process.

#### Suitability of Methods to Address Research Questions:

The proposed methods are well-suited to address the research question of assessing the trustworthiness of explainable AI methods in predictive process analytics:

##### Functionally-Grounded Metrics:

The evaluation uses stability and fidelity assessment methods which eliminate user-based tests for an objective framework resulting in precise reproducible results.

##### Use of XGBoost:

The effectiveness of explainable methods can be effectively tested using XGBoost which demonstrates high accuracy in predictive assignments.

##### Real-World Datasets:

The analysis of real-world event logs enables researchers to establish findings which translate effectively to business process operations.

#### Noteworthy Innovative Approaches

##### Functionally-Grounded Evaluation Metrics:

This paper develops specific evaluation metrics for event log datasets to overcome the deficiencies of standard XAI assessment tools which were created for texts and images.

##### Stability Metrics:

• Stability by Subset $\phi(Z)$ measures similarity in feature subsets across explanations using the following formula: $$\phi(Z)={1-1/d} \sum^d_{i=1} s^{2}_{f_i}/ k.d(1-k/d)$$

1.  • where d is the number of features, $s^2_{f_i}$ is sample variance, and k is the average number of selected features. • Stability by Weight $\phi(W)$ measures consistency in feature weights: $$ \phi(W)={1-1/d} \sum^d_{i=1} \sigma^{2}{w_i}/ |\mu{w_i}|$$ where $\sigma^2_{w_i}$ is variance and $\mu_{w_i}$ is mean weight for feature ii.

2.  Fidelity Metric: • Internal fidelity is quantified using MAPE:$$ F=1/|X'| \sum_{x'\epsilon X'} |Y(x)-Y(x')|/Y(x) $$ where Y(x)and Y(x′) are prediction probabilities for original and perturbed inputs, respectively.

3.  Combination of Encoding and Bucketing Methods: • The use of aggregate encoding with prefix-length bucketing or no bucketing provides insights into how data preprocessing impacts explainability.

#### Key Findings and Contributions

##### 1. Novel Evaluation Framework

A functionally-grounded evaluation system receives special endorsement from the authors to assess explainable methods in predictive process analytics. Existing research lacked evaluation methods which specifically supported assessable explainable methods across different prediction problems according to this proposed framework.

##### 2.Stability Metrics

Two stability metrics were introduced:

a)  Stability by Subset (φ(Z)): $$\phi(Z)={1-1/d} \sum^d_{i=1} s^{2}_{f_i}/ k.d(1-k/d)$$ This metric measures the consistency of feature subsets across explanations.
b)  Stability by Weight (φ(W)): $$ \phi(W)={1-1/d} \sum^d_{i=1} \sigma^{2}{w_i}/ |\mu{w_i}|$$ This metric assesses the stability of feature importance weights.

<!-- -->

##### 3.Internal Fidelity Metric

The authors propose a fidelity measure (F) based on Mean Absolute Percentage Error: $$ F=1/|X'| \sum_{x'\epsilon X'} |Y(x)-Y(x')|/Y(x) $$

This metric quantifies how faithfully the explanation reflects the black-box model's decision-making process.

##### 4. Comprehensive Evaluation:

The researchers examine LIME and SHAP as popular explainable AI techniques for their interpretation of XGBoost models that predict process outcomes. Three actual event log datasets served as research platforms to assess these methods in real-world execution settings.

##### 5. Data Encoding and Bucketing Analysis:

Researchers examine how various data encoding methods (aggregate and index-based) alongside bucketing techniques (prefix-length bucketing and no bucketing) affect explanation quality.

#### Significance in the Broader Context

##### Addressing the Transparency Gap

Due to increasing reliance on complex machine learning models in predictive process analytics the landscape demands stronger transparency and interpretability features. The study specifically aims to solve this problem by offering an assessment framework for clarifying techniques which promotes both AI-driven process analytics transparency and trustworthiness.

##### Standardization of Evaluation

The research introduces performance metrics that depend on functional quality to standardize explainable method assessments in process analytics. Standardization provides essential tools for field advancement through method comparison.

##### Bridging Theory and Practice

Real-world dataset analysis integrations with XGBoost machine learning techniques drive forces between theoretical explainable AI learning and actual process analytics implementations. The applied methods deliver instant results that benefit starkly practical implementation across industry. Foundation for User-Oriented Evaluation

The research creates essential foundational work which will enable upcoming human user evaluations. Effective user studies depend on comprehensive comprehension of technical explanation quality.

##### Guidance for Practitioners

This research evaluates how two common explainable methods (LIME and SHAP) perform when used in process analytics applications. The identified guidelines offer crucial help to practitioners who select methodologies based on their individual requirements.

##### Data Preprocessing Insights

Data preprocessing insights become clearer through the examination of various bucketing and encoding methods which affects explainability outputs. Designing predictive process analytics systems requires effective preprocessing decisions which directly influence both explanation quality and interpretability.

##### Extensibility and Future Research

Two key extension capabilities of the designed metrics and evaluation framework enable their application to new explainable methods that arise during predictive process analytics research. The research will maintain its value because this proactive outlook means it stays aligned with current needs.

##### Interdisciplinary Impact

This research adds value to explainable artificial intelligence by showing how specific process analytics parameters can enhance XAI evaluation frameworks. This method sets an example which might inspire parallel domain-based evaluation techniques across various fields.

#### Implications of the findings for future research or practice:

##### Standardization of Evaluation Metrics

The proposed functionally-grounded metrics for stability and fidelity provide a foundation for standardizing the evaluation of explainable methods in process analytics. These metrics include: Stability by Subset, Stability by Weight, Internal Fidelity

The newly developed metrics facilitate better evaluation of explainable approaches leading to faster field advancement.

Data Preprocessing Considerations

Tests show that how data encoding and bucketing practices function impacts explanation quality. The study explored three combinations:

| Encoding Method | Bucketing Method |
|-----------------|------------------|
| Aggregate       | Prefix-length    |
| Index-based     | Prefix-length    |
| Aggregate       | No bucketing     |

Proper preprocessing within predictive process analytics pipelines requires immediate attention because of the discovered inconsistencies. Research on the discovery of optimal preprocessing techniques for varying process types and predictive challenges should be a focus for upcoming studies.

##### Expanding Evaluation Frameworks

While this study focuses on stability and fidelity, future research could expand the evaluation framework to include other dimensions such as Comprehensibility, Action ability, Computational efficiency

Future research using this functionally-grounded approach should conduct assessments with end-users to evaluate explanation capabilities in real-world practical usage.

##### Integration with Process Mining

The investigation of future research should study methods through which explainable predictive analytics can combine with process discovery techniques to deliver complete business process insights. The merger between explainable predictive models and process mining techniques has the potential to create an integrated approach to analysis and process transformation.

##### Ethical and Responsible AI

The evaluation framework shows promise for adding fairness analysis alongside bias detection for process predictions and explanations while establishing responsible AI standards for process management. The increasing regulatory focus on AI explainability demands a foundation which will enable the development of dedicated compliance frames for predictive process analytics.

This research simultaneously pushes forward existing explainable predictive process analytical methods while establishing multiple future research possibilities and practical usage avenues. Researchers together with practitioners must focus on these regions to develop trustworthy along with transparent AI-driven process analytics solutions which provide effective results.

#### Connection to Other Work

This paper by Velmurugan et al. builds on and extends previous work in explainable AI and predictive process analytics in several key ways:

##### Extension of XAI Evaluation to Process Analytics

The authors applied existing evaluation frameworks for XAI to predictive process analytics while making adjustable modifications to better fit this domain. Building from Doshi-Velez and Kim's (2017) three-level evaluation framework they concentrate their analysis on functionally-grounded evaluation practices. The authors extend existing XAI evaluation practices through applying their principles to specific process data requirements.

Novel Stability Metrics

The paper introduces two stability metrics adapted for process analytics:

Stability by Subset (φ(Z)): The metric utilizes modifications made to Nogueira et al. (2018) feature selection stability framework which operates within explainable process analytics boundaries.

Stability by Weight (φ(W)): The newly developed metric calculates feature weight stability through relative variance analysis.

These metrics extend existing stability measurements from XAI fields by specifically supporting tabular structures and time-related characteristics of process data.

##### Internal Fidelity Metric

The authors introduce an internal fidelity assessment that uses Mean Absolute Percentage Error (MAPE) as its foundation. This differs from previous approaches like:

Researchers used white-box models according to Ribeiro et al. (2016) to measure fidelity.

The researchers Alvarez-Melis and Jaakkola (2018) removed features through an ablation technique to evaluate system fidelity.

The newly designed computational method specializes in analyzing process analytics data while fixing the shortcomings of present measurement techniques that target images and text.

##### Comprehensive Evaluation Framework

The authors introduce a united stability-fidelity framework through their work which delivers stronger evaluation capabilities than research on explainable process analytics from Galanti et al. (2020) and Rehse et al. (2019), that solely engaged in XAI method usage without extensive evaluation.

##### Integration with State-of-the-Art Process Prediction

The research expands upon research by Teinemaa et al. (2019) which validated XGBoost as an optimal algorithm for predicting outcomes by assessing LIME and SHAP explanations for XGBoost models.

##### Data Encoding and Bucketing Analysis

This analysis investigates the effect of diverse encoding and bucketing approaches on explainability quality based on the work performed by Teinemaa et al. (2019) regarding prediction accuracy enhancements from these techniques.

##### Key seminal works cited include:

Doshi-Velez and Kim (2017) on XAI evaluation frameworks

Ribeiro et al. (2016) introducing LIME

Lundberg and Lee (2017) introducing SHAP

#### Relevance to Capstone Project:

Focus on Predictive Analytics: Historical data predictive modeling fits the paper's predictive process analytics focus that mirrors ability to make predictions from historical information.

Use of Advanced Machine Learning: XGBoost serves as the primary tool for model building within this study that directly benefits project's machine learning element.

Evaluation of Explainable AI Methods: A comparative analysis of the explainable AI techniques LIME and SHAP takes place within this study. Including explainable aspects in predictive models can result in major improvements for their overall reliability and worth.

##### Specific Methods and Theories to Incorporate:

Evaluation Metrics: The paper proposes functionally-grounded evaluation metrics for assessing explainable methods:

Stability by Subset Stability by Weight Internal Fidelity

Data Preprocessing Techniques: The paper presents several data encoding and bucketing approaches suitable for preparing historic data.

Aggregate encoding with prefix-length bucketing, Index-based encoding with prefix-length bucketing, Aggregate encoding with no bucketing

Use of Real-World Datasets: The research points to three open-source and real-event systems documenting all workforce activities. The use of multiple sources in authentic datasets works ideally with your historical data analytic requirements.

Potential Areas for Expansion or Divergence:

Domain-Specific Adaptations: Although a process analytics approach guides the research your team could make targeted adjustments to apply their techniques to historical data analysis.

Additional ML Algorithms: The paper primarily uses XGBoost. The study could benefit from extension by performing a comparison between different predictive machine learning algorithms as part of its methodology.

Human-Grounded Evaluation: Evaluation takes place through applying functional criteria in this paper. Your project may add human-grounded assessment techniques to evaluate how practical the explanations from your models prove in actual applications.

Temporal Aspects: Your predictive models require stronger focus on temporal dimensions throughout the dataset which can lead to developing innovative features and time-based analysis approaches.

### Refrences

M. Velmurugan, C. Ouyang, C. Moreira, and R. Sindhgatta, "Evaluating explainable methods for predictive process analytics: A functionally-grounded approach," arXiv preprint arXiv:2012.04218, 2020.

F. Doshi-Velez and B. Kim, "Towards a rigorous science of interpretable machine learning," arXiv preprint arXiv:1702.08608, 2017.

D. Alvarez-Melis and T. S. Jaakkola, "On the robustness of interpretability methods," arXiv preprint arXiv:1806.08049, 2018.
