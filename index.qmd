---
title: "Capstone Project"
format:
  html:
    code-fold: true
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Literature Review 
Background/Motivation

Context in Which the Research Was Conducted:
The use of machine learning (ML) to conceptual modeling (CM), a critical component of software and systems engineering, is the main emphasis of this work. 
Conceptual models are used to graphically depict domain-specific knowledge, such as system designs or business procedures. Combining ML and CM, or ML4CM, 
(Ali, Gavric, Proper, & Bork, 2023) has garnered attention recently as a way to automate processes such as finishing incomplete models, classifying them, or converting them into different formats. 
We must express these models' structure (the relationships between elements) and meaning (the semantics) in a fashion that machine learning algorithms can 
understand 
if we want ML to function effectively with them.

Problem or Gap in the Existing Literature:
The lack of a standard method for encoding conceptual models for machine learning tasks is a significant issue. The majority of current research employs techniques that 
are limited to addressing a single issue, making them challenging to generalize to other contexts. Structural encodings, which take into consideration the linkages and 
connections in models, are frequently disregarded in favor of semantic encodings, such as the usage of text labels. Furthermore, not many methods include data from 
ontologies or metamodels, which could offer more context and more accurately depict the connections between model components. These shortcomings hinder machine learning's 
ability to properly utilize conceptual models for increasingly intricate and varied tasks.

Significance of the Research Question
Improving ML’s Ability to Learn from Models:
The amount of valuable information available to machine learning algorithms during training depends on how conceptual models are encoded. ML finds it difficult to 
identify patterns or provide precise predictions in the absence of rich and meaningful encodings. The goal of this project is to enhance the way machine learning (ML) 
works with conceptual models by examining various encoding techniques, which will increase the efficiency of tasks like transformation and classification (Ali, Gavric, Proper, & Bork, 2023).
Providing Practical Guidance:
The purpose of this study is to assist developers and researchers in determining which encoding techniques are most effective for various ML tasks in CM. The field 
will 
advance more quickly as a result of this clarity, which will make it simpler for others to create and use superior solutions.

Meeting Industry Needs:
Automating processes like model transformation and classification is becoming more and more crucial as model-driven engineering is used in more sectors. Businesses can 
save time, cut down on errors, and enhance the overall quality of their systems by encoding conceptual models in a methodical manner.

Summary of Methods Used

In order to investigate how conceptual models are encoded for machine learning (ML) applications, the authors carried out a Systematic Literature Review (SLR). Several crucial steps were engaged 
in their process:

Establishing the Research Focus: The study sought to investigate:

the kinds of data from conceptual models that are utilized in machine learning training.
strategies for encoding this data.
How encodings fit into particular modeling languages and machine learning tasks.
the connection between encodings and machine learning models.

Search and Selection:
A comprehensive search using a predefined query initially identified 647 papers. After applying rigorous selection criteria—such as excluding studies not focused on 
encoding conceptual models for machine learning—37 papers were ultimately selected.
Categorization and Analysis:
Every chosen paper was categorized according to:
1) The kind of data that is encoded (structural or semantic, for example).
2) Encoding methods such as graph kernels, TF-IDF, and bare graphs.
3) The ML application's goal (e.g., transformation, categorization, or completion).
4) The kind of machine learning models used.
Data Review:
The study explored how different machine learning tasks are handled by comparing explicit and implicit encoding techniques. It also examined the connections between 
machine learning models, their goals, and the methods used for encoding, shedding light on how these components interact in practice.

Suitability of the Methods:
The authors’ decision to use a systematic literature review (SLR) was a perfect match for their research goals. This method allowed them to dive deeply into existing studies,
uncover trends, organize methods, and identify gaps in how machine learning is applied to conceptual modeling. The step-by-step approach ensured their findings were thorough and dependable.

What stood out most about their approach was:

Capturing diversity: They examined a wide range of encoding methods, from straightforward metrics to complex graph-based representations.
Task-specific insights: By connecting encoding techniques to specific machine learning tasks, the study offered valuable, practical guidance for future research and applications.

Noteworthy Contributions and Innovations:
The study made several important contributions to the field. One of the key achievements was creating a simple and clear system to classify encoding methods and their use cases. 
This framework helps bring order and understanding to what can be a very complicated area of research (Ali, Gavric, Proper, & Bork, 2023).
The authors also focused on practical applications by showing how specific encoding methods work well for certain machine learning tasks. For example, they explained how raw graphs are a 
good fit for graph neural networks in tasks like model completion. This makes their findings useful and directly applicable to real-world problems.
Another highlight was their focus on areas that are often overlooked. They pointed out that concepts like metamodels and ontological semantics are rarely used in current approaches. By bringing 
attention to this, the study encourages future research to explore these ideas and improve how models are encoded.
Lastly, the study identified several gaps in the field, such as the lack of standardized datasets and the limited use of advanced methods like graph kernels. These insights provide a clear direction 
for future studies to address these challenges and move the field forward.

Significance of the Work
Key Findings and Contributions:
The study breaks down different ways of encoding conceptual models into simple categories. It looks at semantic methods like TF-IDF and word embeddings and structural ones like raw graphs.  

One key takeaway is that the type of encoding you use really depends on the task. For instance, raw graphs work well with graph neural networks for things like model completion. It shows how important it is to match the method to the job.  

The research also points out some gaps, like not having enough standardized datasets to compare results or fully exploring metamodel and ontological semantics. These are exciting areas where future research can make a big impact.  

Lastly, the authors put together a practical framework that connects encoding methods to specific ML tasks. It’s a handy guide for anyone working in this area to pick the right approach for what they’re trying to do.

Importance in the Field:
This study addresses a crucial gap in understanding how machine learning can be applied to conceptual modeling. By organizing and clarifying different encoding approaches, it lays the groundwork for creating more effective and tailored ML solutions that fit specific tasks.

Implications for Future Research and Practice:     

Shared Datasets: Creating shared datasets for machine learning in conceptual modeling would make it easier for researchers to compare results and reproduce studies.  
More Context in Encodings: Using metamodel and ontological semantics could make encodings more detailed and meaningful.  
New Applications: The study’s insights could lead to new uses for machine learning in conceptual modeling, like building better tools to check or fix models.

Connection to Other Work:
This paper is part of the growing research on how machine learning (ML) and conceptual modeling (CM) come together. It builds on earlier work by breaking down encoding methods and showing how they’re used in different ML tasks—a level of detail that hasn’t really been done in a systematic way before.

Building on Previous Work:
Structured Review Approach: Unlike earlier studies that focused on specific techniques or single tasks, this paper takes a broader and more organized look at 
the topic. For example, while studies like Lopez et al. compared ML classification methods, this one goes deeper, looking at the types of information used for encoding, like metamodel and ontological semantics.  

Task-Driven Insights: Instead of just looking at encoding methods on their own, this paper shows how they’re connected to specific ML tasks, like model completion or classification. This makes the findings more useful and easier to apply.  

References to Influential Work:
To bolster their findings, the authors cite a number of significant studies in the fields of machine learning (ML) and conceptual modeling (CM). 
These consist of: 
The methodology of Kitchenham and Charters' investigation was greatly influenced by the systematic literature review criteria. 
The foundation for comprehending the use of ML classification algorithms to CM was established by Lopez et al.'s study. 
Many of the encoding techniques examined in this study have technical roots in early research on TF-IDF, word embeddings, and graph neural networks. 

Relevance to Capstone Project:
The content of the study provides insights into how conceptual models may be encoded to enhance ML applications, which is highly beneficial to our capstone thesis on machine learning and predictive analytics. The link is as follows: 
If your project involves historical data in organized formats (such databases or models), the study's description of structural and semantic encodings, including raw graphs and TF-IDF, may help you prepare data for machine learning models. 
Task-specific modeling, which entails matching encoding techniques to specific tasks like classification or prediction, is the main emphasis of the study. With the help of this strategy, (Ali, Gavric, Proper, & Bork, 2023) you may decide how to arrange your data and select encoding methods that will help you achieve your anticipated goals.
Leveraging Semantics: To increase model accuracy and interpretability, you may want to investigate the usage of semantic encodings, like word embeddings, if your data contains descriptive elements (such as natural language labels or metadata).
Opportunities and Gaps: The study identifies some gaps, such as the underutilization of ontological semantics, which may encourage you to look into novel approaches of incorporating domain information into your prediction models.


## References

Ali, S. J., Gavric, A., Proper, H., & Bork, D. (2023, October). Encoding Conceptual Models for Machine Learning: A Systematic Review. In 2023 ACM/IEEE International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C) (pp. 562-570). IEEE.